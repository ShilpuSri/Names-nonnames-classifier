{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "#import dga_classifier.data as data\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df3=pd.read_csv('../input/englishdataset/Dataset_English.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aisha</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aishah</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ajay</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaisha</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaishah</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568820</th>\n",
       "      <td>ayzlyn</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568821</th>\n",
       "      <td>rozzlyn</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568822</th>\n",
       "      <td>soffie</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568823</th>\n",
       "      <td>valaysia</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568824</th>\n",
       "      <td>bryxton</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568825 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Words Label\n",
       "0          aisha  name\n",
       "1         aishah  name\n",
       "2           ajay  name\n",
       "3         aaisha  name\n",
       "4        aaishah  name\n",
       "...          ...   ...\n",
       "568820    ayzlyn  name\n",
       "568821   rozzlyn  name\n",
       "568822    soffie  name\n",
       "568823  valaysia  name\n",
       "568824   bryxton  name\n",
       "\n",
       "[568825 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "words=[]\n",
    "count_non_names = 0\n",
    "count_names = 0\n",
    "for x in range(len(df3)):\n",
    "    if df3.iloc[x]['Label'] == 'non-name':\n",
    "        y.append(0)\n",
    "        count_non_names += 1\n",
    "    else:\n",
    "        y.append(1)\n",
    "        count_names += 1\n",
    "    words.append(str(df3.iloc[x]['Words']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.asarray(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 31\n",
    "EMBEDDING_DIM = 300\n",
    "embeddings_index = {}\n",
    "f = open('../input/glove6b/glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.29712     0.094049   -0.096662   ...  0.059717   -0.22853\n",
      "   0.29602   ]\n",
      " [ 0.048902    0.29453    -0.06686    ... -1.46179998 -0.35120001\n",
      "  -0.098858  ]\n",
      " ...\n",
      " [-0.32229    -0.28106001  0.51416999 ... -0.090831    0.43952\n",
      "   0.51388001]\n",
      " [ 0.10812     0.11993    -0.27105999 ... -0.82524002 -0.17184\n",
      "   0.51672   ]\n",
      " [-0.072908    0.42648     0.33423999 ... -0.43575001  0.57086003\n",
      "   0.35189   ]]\n",
      "(27, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((27, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector \n",
    "    \n",
    "print(embedding_matrix)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train and test LSTM classifier\"\"\"\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "def build_model(max_features, maxlen):\n",
    "    \"\"\"Build Bi-LSTM model\"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(max_features, 300, weights=[embedding_matrix], input_length=maxlen, trainable=True))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(300)))\n",
    "    model.add(tf.keras.layers.Dense(300))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "def run(max_epoch=25, nfolds=10, batch_size=128):\n",
    "    \"\"\"Run train/test on logistic regression model\"\"\"\n",
    "    \n",
    "    # Extract data and labels\n",
    "    X = df3[['Words']]\n",
    "    labels = df3[['Label']]\n",
    "    valid_chars = dict(zip(string.ascii_lowercase, range(1,27)))\n",
    "    print(len(valid_chars))\n",
    "    max_features = len(valid_chars) + 1\n",
    "    max_len=[]\n",
    "    for x in words:\n",
    "        max_len.append(len(x))\n",
    "    maxlen = np.max(max_len)\n",
    "    print(maxlen)\n",
    "\n",
    "    X = [[valid_chars[y] for y in x] for x in words]\n",
    "    print(len(X))\n",
    "    X = sequence.pad_sequences(X, maxlen=maxlen)\n",
    "    print(X.shape)\n",
    "    \n",
    "    weights={0: 0.41, 1: 0.59}\n",
    "    \n",
    "    final_data = []\n",
    "\n",
    "    for fold in range(nfolds):\n",
    "        print (\"fold %u/%u\" % (fold+1, nfolds))\n",
    "        X_train, X_test, y_train, y_test, _, label_test = train_test_split(X, y, labels,test_size=0.2)\n",
    " \n",
    "        print(X_train.shape)\n",
    "        print ('Build model...')\n",
    "        model = build_model(max_features, maxlen)\n",
    "\n",
    "        print (\"Train...\")\n",
    "        X_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y_train, test_size=0.05)\n",
    "        best_iter = -1\n",
    "        best_auc = 0.0\n",
    "        out_data = {}\n",
    "\n",
    "        for ep in range(max_epoch):\n",
    "            model.fit(X_train, y_train, batch_size=batch_size, epochs=1, class_weight=weights)\n",
    "\n",
    "            t_probs = model.predict_proba(X_holdout)\n",
    "            t_auc = sklearn.metrics.roc_auc_score(y_holdout, t_probs)\n",
    "\n",
    "            print ('Epoch %d: auc = %f (best=%f)' % (ep, t_auc, best_auc))\n",
    "\n",
    "            if t_auc > best_auc:\n",
    "                best_auc = t_auc\n",
    "                best_iter = ep\n",
    "\n",
    "                probs = model.predict_proba(X_test)\n",
    "                \n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                out_data = {'y':y_test, 'y_pred': y_pred, 'labels': label_test, 'probs':probs, 'epochs': ep,\n",
    "                            'confusion_matrix': sklearn.metrics.confusion_matrix(y_test, probs > .5)}\n",
    "\n",
    "                print (sklearn.metrics.confusion_matrix(y_test, probs > .5))\n",
    "            else:\n",
    "                # No longer improving...break and calc statistics\n",
    "                if (ep-best_iter) > 2:\n",
    "                    break\n",
    "\n",
    "        final_data.append(out_data)\n",
    "        model.save(\"model.h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "\n",
    "        \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "31\n",
      "568825\n",
      "(568825, 31)\n",
      "fold 1/1\n",
      "(455060, 31)\n",
      "Build model...\n",
      "Train...\n",
      "3378/3378 [==============================] - 63s 19ms/step - loss: 0.1465\n",
      "Epoch 0: auc = 0.958518 (best=0.000000)\n",
      "[[58902  7805]\n",
      " [ 4476 42582]]\n",
      "3378/3378 [==============================] - 60s 18ms/step - loss: 0.1140\n",
      "Epoch 1: auc = 0.965186 (best=0.958518)\n",
      "[[57245  9462]\n",
      " [ 2406 44652]]\n",
      "3378/3378 [==============================] - 60s 18ms/step - loss: 0.1004\n",
      "Epoch 2: auc = 0.967843 (best=0.965186)\n",
      "[[58854  7853]\n",
      " [ 3084 43974]]\n",
      "3378/3378 [==============================] - 60s 18ms/step - loss: 0.0895\n",
      "Epoch 3: auc = 0.969922 (best=0.967843)\n",
      "[[58695  8012]\n",
      " [ 2704 44354]]\n",
      "3378/3378 [==============================] - 59s 18ms/step - loss: 0.0796\n",
      "Epoch 4: auc = 0.970483 (best=0.969922)\n",
      "[[59926  6781]\n",
      " [ 3330 43728]]\n",
      "3378/3378 [==============================] - 59s 18ms/step - loss: 0.0696\n",
      "Epoch 5: auc = 0.970946 (best=0.970483)\n",
      "[[60525  6182]\n",
      " [ 3690 43368]]\n",
      "3378/3378 [==============================] - 59s 18ms/step - loss: 0.0608\n",
      "Epoch 6: auc = 0.970843 (best=0.970946)\n",
      "3378/3378 [==============================] - 59s 18ms/step - loss: 0.0522\n",
      "Epoch 7: auc = 0.970359 (best=0.970946)\n",
      "3378/3378 [==============================] - 60s 18ms/step - loss: 0.0449\n",
      "Epoch 8: auc = 0.970396 (best=0.970946)\n",
      "Saved model to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:79: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run experiments and create figs\"\"\"\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "RESULT_FILE = 'results.pkl'\n",
    "\n",
    "def run_experiments(islstm=True, nfolds=10):\n",
    "    \"\"\"Runs all experiments\"\"\"\n",
    "    #bigram_results = None\n",
    "    lstm_results = None\n",
    "\n",
    "    #if isbigram:\n",
    "    #    bigram_results = bigram.run(nfolds=nfolds)\n",
    "\n",
    "    if islstm:\n",
    "        lstm_results = run(nfolds=nfolds)\n",
    "\n",
    "    return lstm_results\n",
    "\n",
    "def create_figs(islstm=True, nfolds=10, force=False):\n",
    "    \"\"\"Create figures\"\"\"\n",
    "    # Generate results if needed\n",
    "    if force or (not os.path.isfile(RESULT_FILE)):\n",
    "        lstm_results = run_experiments(islstm, nfolds)\n",
    "\n",
    "        results = {'lstm': lstm_results}\n",
    "\n",
    "        pickle.dump(results, open(RESULT_FILE, 'wb'))\n",
    "    else:\n",
    "        results = pickle.load(open(RESULT_FILE,'rb'))\n",
    "\n",
    "\n",
    "    # xtract and calculate LSTM ROC\n",
    "    if results['lstm']:\n",
    "        lstm_results = results['lstm']\n",
    "        fpr = []\n",
    "        tpr = []\n",
    "        for lstm_result in lstm_results:\n",
    "            t_fpr, t_tpr, _ = roc_curve(lstm_result['y'], lstm_result['probs'])\n",
    "            fpr.append(t_fpr)\n",
    "            tpr.append(t_tpr)\n",
    "        lstm_binary_fpr, lstm_binary_tpr, lstm_binary_auc = calc_macro_roc(fpr, tpr)\n",
    "\n",
    "    # Save figure\n",
    "    from matplotlib import pyplot as plt\n",
    "    with plt.style.context('bmh'):\n",
    "        plt.plot(lstm_binary_fpr, lstm_binary_tpr,\n",
    "                 label='LSTM (AUC = %.4f)' % (lstm_binary_auc, ), rasterized=True)\n",
    "        #plt.plot(bigram_binary_fpr, bigram_binary_tpr,\n",
    "        #         label='Bigrams (AUC = %.4f)' % (bigram_binary_auc, ), rasterized=True)\n",
    "\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate', fontsize=22)\n",
    "        plt.ylabel('True Positive Rate', fontsize=22)\n",
    "        plt.title('ROC - Binary Classification', fontsize=26)\n",
    "        plt.legend(loc=\"lower right\", fontsize=22)\n",
    "\n",
    "        plt.tick_params(axis='both', labelsize=22)\n",
    "        plt.savefig('results.png')\n",
    "\n",
    "def calc_macro_roc(fpr, tpr):\n",
    "    \"\"\"Calcs macro ROC on log scale\"\"\"\n",
    "    # Create log scale domain\n",
    "    all_fpr = sorted(itertools.chain(*fpr))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(len(tpr)):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    return all_fpr, mean_tpr / len(tpr), auc(all_fpr, mean_tpr) / len(tpr)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_figs(nfolds=1) # Run with 1 to make it fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('results.pkl', 'rb')\n",
    "\n",
    "# dump information to that file\n",
    "data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data['lstm'][0]['probs']\n",
    "x1=data['lstm'][0]['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "for i in x:\n",
    "    if i > 0.5:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = [i for i, x in enumerate(x1) if x == 1]\n",
    "index0 = [i for i, x in enumerate(x1) if x == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred0=[]\n",
    "y_pred1=[]\n",
    "for i in index0:\n",
    "    y_pred0.append(y_pred[i])\n",
    "for i in index1:\n",
    "    y_pred1.append(y_pred[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test0=[]\n",
    "y_test1=[]\n",
    "for i in index0:\n",
    "    y_test0.append(x1[i])\n",
    "for i in index1:\n",
    "    y_test1.append(x1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9073260677290239\n",
      "0.9215861277572358\n",
      "0.913224629719158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test0, y_pred0))\n",
    "print(accuracy_score(y_test1, y_pred1))\n",
    "print(accuracy_score(x1, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92     66707\n",
      "           1       0.88      0.92      0.90     47058\n",
      "\n",
      "    accuracy                           0.91    113765\n",
      "   macro avg       0.91      0.91      0.91    113765\n",
      "weighted avg       0.91      0.91      0.91    113765\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(x1, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
